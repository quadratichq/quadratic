import * as AI_RATES from 'quadratic-shared/ai/models/AI_RATES';
import type { AIModelConfig, AIModelKey } from 'quadratic-shared/typesAndSchemasAI';

// updating this will force the model to be reset to the default model in local storage
export const DEFAULT_MODEL_VERSION = 20;

// used when `quadratic:quadratic-auto:thinking-toggle-off` is selected, in model router
export const DEFAULT_MODEL_ROUTER_MODEL: AIModelKey = 'vertexai:gemini-2.5-flash:thinking-toggle-off';

// AI Analyst and AI Assistant chat models
export const DEFAULT_MODEL_PRO: AIModelKey = 'vertexai-anthropic:claude-sonnet-4:thinking';
export const DEFAULT_MODEL_FREE: AIModelKey = 'openai:ft:gpt-4.1-mini-2025-04-14:quadratic::BupmrWcz';

// Backup models for AI Analyst and AI Assistant chat models
export const DEFAULT_BACKUP_MODEL: AIModelKey = 'bedrock-anthropic:claude:thinking-toggle-off';
export const DEFAULT_BACKUP_MODEL_THINKING: AIModelKey = 'bedrock-anthropic:claude:thinking-toggle-on';

// Internal tool call models
export const DEFAULT_GET_CHAT_NAME_MODEL: AIModelKey = 'vertexai:gemini-2.5-flash:thinking-toggle-off';
export const DEFAULT_PDF_IMPORT_MODEL: AIModelKey = 'vertexai:gemini-2.5-flash:thinking-toggle-off';
export const DEFAULT_SEARCH_MODEL: AIModelKey = 'vertexai:gemini-2.5-flash:thinking-toggle-off';
export const DEFAULT_CODE_EDITOR_COMPLETIONS_MODEL: AIModelKey = 'vertexai:gemini-2.5-flash:thinking-toggle-off'; // not used
export const DEFAULT_GET_USER_PROMPT_SUGGESTIONS_MODEL: AIModelKey = 'vertexai:gemini-2.5-flash:thinking-toggle-off'; // not used

export const MODELS_CONFIGURATION: {
  [key in AIModelKey]: AIModelConfig;
} = {
  // uses 'DEFAULT_MODEL_ROUTER_MODEL' to find the best model for this prompt,
  // settings and price correspond to the current 'DEFAULT_MODEL_ROUTER_MODEL'
  'quadratic:quadratic-auto:thinking-toggle-off': {
    model: 'quadratic-auto',
    displayName: 'auto',
    temperature: 0,
    max_tokens: 8192,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'quadratic',
    promptCaching: false,
    thinking: false,
    thinkingToggle: false,
    ...AI_RATES.gemini_2_0_flash_rate,
  },
  'quadratic:quadratic-auto:thinking-toggle-on': {
    model: 'gemini-2.5-pro',
    displayName: 'gemini 2.5 pro',
    temperature: 0,
    max_tokens: 65535,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai',
    promptCaching: false,
    thinking: true,
    thinkingToggle: true,
    thinkingBudget: 32768,
    ...AI_RATES.gemini_2_5_pro_rate,
  },
  'vertexai-anthropic:claude-sonnet-4:thinking': {
    model: 'claude-sonnet-4@20250514',
    displayName: 'claude sonnet 4',
    temperature: 1,
    max_tokens: 64000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'pro',
    provider: 'vertexai-anthropic',
    promptCaching: true,
    thinking: true,
    ...AI_RATES.claude_sonnet_4_20250514_rate,
  },
  'vertexai-anthropic:claude-sonnet-4:thinking-toggle-off': {
    model: 'claude-sonnet-4@20250514',
    displayName: 'claude sonnet 4',
    temperature: 0,
    max_tokens: 64000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai-anthropic',
    promptCaching: true,
    thinking: false,
    thinkingToggle: false,
    ...AI_RATES.claude_sonnet_4_20250514_rate,
  },
  'vertexai-anthropic:claude-sonnet-4:thinking-toggle-on': {
    model: 'claude-sonnet-4@20250514',
    displayName: 'claude sonnet 4',
    temperature: 1,
    max_tokens: 64000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai-anthropic',
    promptCaching: true,
    thinking: true,
    thinkingToggle: true,
    ...AI_RATES.claude_sonnet_4_20250514_rate,
  },
  'vertexai:gemini-2.5-pro:thinking-toggle-off': {
    model: 'gemini-2.5-pro',
    displayName: 'gemini 2.5 pro',
    temperature: 0,
    max_tokens: 65535,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai',
    promptCaching: false,
    thinking: false,
    thinkingToggle: false,
    thinkingBudget: 128,
    ...AI_RATES.gemini_2_5_pro_rate,
  },
  'vertexai:gemini-2.5-pro:thinking-toggle-on': {
    model: 'gemini-2.5-pro',
    displayName: 'gemini 2.5 pro',
    temperature: 0,
    max_tokens: 65535,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai',
    promptCaching: false,
    thinking: true,
    thinkingToggle: true,
    thinkingBudget: 32768,
    ...AI_RATES.gemini_2_5_pro_rate,
  },
  'vertexai:gemini-2.5-pro:thinking': {
    model: 'gemini-2.5-pro',
    displayName: 'gemini 2.5 pro',
    temperature: 0,
    max_tokens: 65535,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai',
    promptCaching: false,
    thinking: true,
    thinkingBudget: 32768,
    ...AI_RATES.gemini_2_5_pro_rate,
  },
  'vertexai:gemini-2.5-flash:thinking-toggle-off': {
    model: 'gemini-2.5-flash',
    displayName: 'gemini 2.5 flash',
    temperature: 0,
    max_tokens: 65535,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai',
    promptCaching: false,
    thinking: false,
    thinkingBudget: 0,
    ...AI_RATES.gemini_2_5_flash_rate,
  },
  'vertexai:gemini-2.5-flash:thinking-toggle-on': {
    model: 'gemini-2.5-flash',
    displayName: 'gemini 2.5 flash',
    temperature: 0,
    max_tokens: 65535,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai',
    promptCaching: false,
    thinking: true,
    thinkingToggle: true,
    thinkingBudget: 24576,
    ...AI_RATES.gemini_2_5_flash_rate,
  },
  'vertexai:gemini-2.0-flash': {
    model: 'gemini-2.0-flash',
    displayName: 'gemini 2.0 flash',
    temperature: 0,
    max_tokens: 8192,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'vertexai',
    promptCaching: false,
    thinking: false,
    ...AI_RATES.gemini_2_0_flash_rate,
  },
  'geminiai:gemini-2.5-flash-lite-preview-06-17': {
    model: 'gemini-2.5-flash-lite-preview-06-17',
    displayName: 'gemini 2.5 flash lite',
    temperature: 0,
    max_tokens: 65535,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'geminiai',
    promptCaching: false,
    thinking: false,
    thinkingBudget: 0,
    ...AI_RATES.gemini_2_5_flash_lite_rate,
  },
  'bedrock-anthropic:claude-sonnet-4:thinking-toggle-off': {
    model: 'us.anthropic.claude-sonnet-4-20250514-v1:0',
    displayName: 'claude sonnet 4',
    temperature: 0,
    max_tokens: 32000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'bedrock-anthropic',
    promptCaching: true,
    thinking: false,
    thinkingToggle: false,
    ...AI_RATES.claude_sonnet_4_20250514_rate,
  },
  'bedrock-anthropic:claude-sonnet-4:thinking-toggle-on': {
    model: 'us.anthropic.claude-sonnet-4-20250514-v1:0',
    displayName: 'claude sonnet 4',
    temperature: 1,
    max_tokens: 32000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'bedrock-anthropic',
    promptCaching: true,
    thinking: true,
    thinkingToggle: true,
    ...AI_RATES.claude_sonnet_4_20250514_rate,
  },
  'bedrock-anthropic:claude:thinking-toggle-off': {
    model: 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',
    displayName: 'claude',
    temperature: 0,
    max_tokens: 8192,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'bedrock-anthropic',
    promptCaching: false,
    thinking: false,
    thinkingToggle: false,
    ...AI_RATES.claude_sonnet_3_5_20250514_rate,
  },
  'bedrock-anthropic:claude:thinking-toggle-on': {
    model: 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',
    displayName: 'claude',
    temperature: 1,
    max_tokens: 16000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'bedrock-anthropic',
    promptCaching: true,
    thinking: true,
    thinkingToggle: true,
    ...AI_RATES.claude_sonnet_3_7_20250514_rate,
  },
  'bedrock-anthropic:us.anthropic.claude-3-7-sonnet-20250219-v1:0:thinking-toggle-off': {
    model: 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',
    displayName: 'claude 3.7 sonnet',
    temperature: 0,
    max_tokens: 8192,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'bedrock-anthropic',
    promptCaching: true,
    thinking: false,
    thinkingToggle: false,
    ...AI_RATES.claude_sonnet_3_7_20250514_rate,
  },
  'bedrock-anthropic:us.anthropic.claude-3-7-sonnet-20250219-v1:0:thinking-toggle-on': {
    model: 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',
    displayName: 'claude 3.7 sonnet',
    temperature: 1,
    max_tokens: 16000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'bedrock-anthropic',
    promptCaching: true,
    thinking: true,
    thinkingToggle: true,
    ...AI_RATES.claude_sonnet_3_7_20250514_rate,
  },
  'bedrock:us.deepseek.r1-v1:0': {
    model: 'us.deepseek.r1-v1:0',
    displayName: 'deepseek r1',
    temperature: 0,
    max_tokens: 32768,
    canStream: true,
    canStreamWithToolCalls: false,
    mode: 'disabled',
    provider: 'bedrock',
    promptCaching: false,
    thinking: false,
    rate_per_million_input_tokens: 1.35,
    rate_per_million_output_tokens: 5.4,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'anthropic:claude-sonnet-4:thinking-toggle-off': {
    model: 'claude-sonnet-4-20250514',
    displayName: 'claude sonnet 4',
    temperature: 0,
    max_tokens: 64000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'anthropic',
    promptCaching: true,
    thinking: false,
    thinkingToggle: false,
    ...AI_RATES.claude_sonnet_4_20250514_rate,
  },
  'anthropic:claude-sonnet-4:thinking-toggle-on': {
    model: 'claude-sonnet-4-20250514',
    displayName: 'claude sonnet 4',
    temperature: 1,
    max_tokens: 64000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'anthropic',
    promptCaching: true,
    thinking: true,
    thinkingToggle: true,
    ...AI_RATES.claude_sonnet_4_20250514_rate,
  },
  'anthropic:claude:thinking-toggle-off': {
    model: 'claude-3-5-sonnet-20241022',
    displayName: 'claude',
    temperature: 0,
    max_tokens: 8192,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'anthropic',
    promptCaching: true,
    thinking: false,
    thinkingToggle: false,
    ...AI_RATES.claude_sonnet_3_5_20250514_rate,
  },
  'anthropic:claude:thinking-toggle-on': {
    model: 'claude-3-7-sonnet-20250219',
    displayName: 'claude',
    temperature: 1,
    max_tokens: 16000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'anthropic',
    promptCaching: true,
    thinking: true,
    thinkingToggle: true,
    ...AI_RATES.claude_sonnet_3_7_20250514_rate,
  },
  'openai:gpt-4.1-2025-04-14': {
    model: 'gpt-4.1-2025-04-14',
    displayName: 'gpt 4.1',
    temperature: 0,
    max_tokens: 32768,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'openai',
    promptCaching: true, // not used for openai, managed by the api
    strictParams: false,
    rate_per_million_input_tokens: 2,
    rate_per_million_output_tokens: 8,
    rate_per_million_cache_read_tokens: 0.5,
    rate_per_million_cache_write_tokens: 0,
  },
  'openai:ft:gpt-4.1-2025-04-14:quadratic::BvusunQW': {
    model: 'ft:gpt-4.1-2025-04-14:quadratic::BvusunQW',
    displayName: 'quad-ft-BvusunQW',
    temperature: 0.1,
    max_tokens: 16384,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'openai',
    promptCaching: true, // not used for openai, managed by the api
    strictParams: false,
    rate_per_million_input_tokens: 3,
    rate_per_million_output_tokens: 12,
    rate_per_million_cache_read_tokens: 0.75,
    rate_per_million_cache_write_tokens: 0,
  },
  'openai:ft:gpt-4.1-mini-2025-04-14:quadratic::BupmrWcz': {
    model: 'ft:gpt-4.1-mini-2025-04-14:quadratic::BupmrWcz',
    displayName: 'quad-mini-ft-BupmrWcz',
    temperature: 0.1,
    max_tokens: 16384,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'basic',
    provider: 'openai',
    promptCaching: true, // not used for openai, managed by the api
    strictParams: false,
    rate_per_million_input_tokens: 0.8,
    rate_per_million_output_tokens: 3.2,
    rate_per_million_cache_read_tokens: 0.2,
    rate_per_million_cache_write_tokens: 0,
  },
  'openai:ft:gpt-4.1-nano-2025-04-14:quadratic::BuLAij8n': {
    model: 'ft:gpt-4.1-nano-2025-04-14:quadratic::BuLAij8n',
    displayName: 'quad-nano-ft-BuLAij8n',
    temperature: 0.1,
    max_tokens: 16384,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'openai',
    promptCaching: true, // not used for openai, managed by the api
    strictParams: false,
    rate_per_million_input_tokens: 0.2,
    rate_per_million_output_tokens: 0.8,
    rate_per_million_cache_read_tokens: 0.05,
    rate_per_million_cache_write_tokens: 0,
  },
  'openai:gpt-4.1-mini-2025-04-14': {
    model: 'gpt-4.1-mini-2025-04-14',
    displayName: 'gpt 4.1 mini',
    temperature: 0,
    max_tokens: 32768,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'openai',
    promptCaching: true, // not used for openai, managed by the api
    strictParams: false,
    rate_per_million_input_tokens: 0.4,
    rate_per_million_output_tokens: 1.6,
    rate_per_million_cache_read_tokens: 0.1,
    rate_per_million_cache_write_tokens: 0,
  },
  'openai:o4-mini-2025-04-16': {
    model: 'o4-mini-2025-04-16',
    displayName: 'o4 mini',
    temperature: 1, // only temperature 1 is supported for o1
    max_tokens: 100000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'openai',
    promptCaching: true, // not used for openai, managed by the api
    strictParams: false,
    rate_per_million_input_tokens: 1.1,
    rate_per_million_output_tokens: 4.4,
    rate_per_million_cache_read_tokens: 0.275,
    rate_per_million_cache_write_tokens: 0,
  },
  'openai:o3-2025-04-16': {
    model: 'o3-2025-04-16',
    displayName: 'o3',
    temperature: 1, // only temperature 1 is supported for o1
    max_tokens: 100000,
    canStream: false, // stream is not supported for o1
    canStreamWithToolCalls: false,
    mode: 'disabled',
    provider: 'openai',
    promptCaching: true, // not used for openai, managed by the api
    strictParams: false,
    rate_per_million_input_tokens: 10,
    rate_per_million_output_tokens: 40,
    rate_per_million_cache_read_tokens: 2.5,
    rate_per_million_cache_write_tokens: 0,
  },
  'azure-openai:gpt-4.1': {
    model: 'gpt-4.1',
    displayName: 'gpt 4.1',
    temperature: 0,
    max_tokens: 32768,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'azure-openai',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 2,
    rate_per_million_output_tokens: 8,
    rate_per_million_cache_read_tokens: 0.5,
    rate_per_million_cache_write_tokens: 0,
  },
  'azure-openai:gpt-4.1-mini': {
    model: 'gpt-4.1-mini',
    displayName: 'gpt 4.1 mini',
    temperature: 0,
    max_tokens: 32768,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'azure-openai',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.4,
    rate_per_million_output_tokens: 1.6,
    rate_per_million_cache_read_tokens: 0.1,
    rate_per_million_cache_write_tokens: 0,
  },
  'xai:grok-4-0709': {
    model: 'grok-4-0709',
    displayName: 'grok 4',
    temperature: 0,
    max_tokens: 256000,
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'xai',
    promptCaching: true, // not used for xai
    strictParams: false,
    rate_per_million_input_tokens: 3,
    rate_per_million_output_tokens: 15,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'baseten:moonshotai/Kimi-K2-Instruct': {
    model: 'moonshotai/Kimi-K2-Instruct',
    displayName: 'Kimi K2 Instruct',
    temperature: 0,
    max_tokens: 0, // use api default
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'baseten',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.6,
    rate_per_million_output_tokens: 2.5,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'open-router:qwen/qwen3-32b': {
    model: 'qwen/qwen3-32b',
    displayName: 'qwen3-32b',
    temperature: 0,
    max_tokens: 0, // use api default
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'open-router',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.1,
    rate_per_million_output_tokens: 0.3,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'open-router:qwen/qwen3-235b-a22b': {
    model: 'qwen/qwen3-235b-a22b',
    displayName: 'qwen3-235b-a22b',
    temperature: 0,
    max_tokens: 0, // use api default
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'open-router',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.13,
    rate_per_million_output_tokens: 0.6,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'open-router:deepseek/deepseek-r1-0528': {
    model: 'deepseek/deepseek-r1-0528',
    displayName: 'deepseek r1 0528',
    temperature: 0,
    max_tokens: 0, // use api default
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'open-router',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.5,
    rate_per_million_output_tokens: 2.15,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'open-router:deepseek/deepseek-r1-0528-qwen3-8b': {
    model: 'deepseek/deepseek-r1-0528-qwen3-8b',
    displayName: 'deepseek r1 0528 qwen3-8b',
    temperature: 0,
    max_tokens: 0, // use api default
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'open-router',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.01,
    rate_per_million_output_tokens: 0.02,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'open-router:deepseek/deepseek-r1-distill-qwen-7b': {
    model: 'deepseek/deepseek-r1-distill-qwen-7b',
    displayName: 'deepseek r1 distill qwen-7b',
    temperature: 0,
    max_tokens: 0, // use api default
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'open-router',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.01,
    rate_per_million_output_tokens: 0.02,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'open-router:deepseek/deepseek-chat-v3-0324': {
    model: 'deepseek/deepseek-chat-v3-0324',
    displayName: 'deepseek v3',
    temperature: 0,
    max_tokens: 0, // use api default
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'open-router',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.28,
    rate_per_million_output_tokens: 0.88,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
  'open-router:moonshotai/kimi-k2': {
    model: 'moonshotai/kimi-k2',
    displayName: 'kimi k2',
    temperature: 0,
    max_tokens: 0, // use api default
    canStream: true,
    canStreamWithToolCalls: true,
    mode: 'disabled',
    provider: 'open-router',
    promptCaching: true,
    strictParams: false,
    rate_per_million_input_tokens: 0.5,
    rate_per_million_output_tokens: 1.5,
    rate_per_million_cache_read_tokens: 0,
    rate_per_million_cache_write_tokens: 0,
  },
} as const;
